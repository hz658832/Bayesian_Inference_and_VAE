{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6aa7912",
   "metadata": {},
   "source": [
    "# Variational (Bayesian) Inference \n",
    "\n",
    "Probabilistic model: $p(z, \\theta)=p(z|\\theta)p(\\theta)$\n",
    "\n",
    "Training: $p(\\theta | X_{tr}, Y_{tr}) = \\frac{p(Y_{tr}|X_{tr}, \\theta)p(\\theta)}{\\int{p(Y_{tr} | X_{tr}, \\theta)p(\\theta)d\\theta}}$  ===> math. intractable \n",
    "\n",
    "Testing: $p(y|x, X_{tr}, Y_{tr})=\\int{p(y|x, \\theta)p(\\theta | X_{tr}, Y_{tr})}$ ===> math. intractable \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6a0d08",
   "metadata": {},
   "source": [
    "## MCMC\n",
    "Samples from unnormalized $p(\\theta|z)$\n",
    "- Unbiased\n",
    "- Need a looooooot of samples regarding to state space dimension\n",
    "\n",
    "## Variational Inferernce\n",
    "Instead to approximate of $Posterior~p(\\theta|z)$ directly, approximate $p(\\theta|z) \\approx q(\\theta)$, which can be understanden as kind of Representation of a model. So called latent space.\n",
    "- Biased\n",
    "- Faster and more scalable\n",
    "\n",
    "\n",
    "Latent space: refers to an abstract multi-dimensional space containing feature values that we cannot interpret directly, but which encodes a meaningful internal representation of externally observed events.\n",
    "\n",
    "Main Idea: find posterior approximation $p(\\theta|z) \\approx q(\\theta) \\in \\mathcal{Q}$ using the relative entropy (so called Kullback-Leibler divergence) as criterion function:\n",
    "\n",
    "$L(q):= KL(q(\\theta) || p(\\theta | z))$, where $KL(q || p) \\geq 0$\n",
    "\n",
    "Hint:\n",
    "- Entropy of a distribution: $H = - \\sum_{i=1}^Np(x_i)logp(x_i)$\n",
    "- Kullback-Leibler divergence: $KL(q || p) = \\sum_{i=1}^Np(x_i)\\frac{log(p(x_i))}{log(q(x_i))}$\n",
    "\n",
    "\n",
    "Solution: $L(q):= KL(q(\\theta) || p(\\theta | z)) \\rightarrow min_{q(\\theta) \\in \\mathcal{Q}}$\n",
    "\n",
    "Two problems:\n",
    "1. The posterior in the KL can still not be computed\n",
    "2. How to perform an optimization w.r.t. a distribution?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5613953d",
   "metadata": {},
   "source": [
    "## Problem 1: The posterior in the KL can still not be computed\n",
    "\n",
    "### Magic:\n",
    "\n",
    "$\\log p(z) = \\int q(\\theta)\\log p(z)d\\theta = \\int q(\\theta)\\log \\frac{p(z, \\theta)}{p(\\theta|z)}d\\theta$\n",
    "\n",
    "$= \\int q(\\theta) \\log \\frac{p(z, \\theta)q(\\theta)}{p(\\theta | z)q(\\theta)}d\\theta=$\n",
    "\n",
    "$= \\int q(\\theta) \\log \\frac{p(x,\\theta)}{q(\\theta)}d\\theta + \\int q(\\theta)\\log \\frac{q(\\theta)}{p(\\theta | z)}d\\theta =$\n",
    "\n",
    "$=\\mathcal{L}(q(\\theta))+KL(q(\\theta)||p(\\theta | z))$\n",
    "\n",
    "Here:\n",
    "- $=\\mathcal{L}(q(\\theta))$ is called Evidence lower bound (ELBO) or Variational lower Bound\n",
    "- $KL(q(\\theta)||p(\\theta | z))$: KL-divergence needed for VI, which is still, intractable...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed4592b",
   "metadata": {},
   "source": [
    "### ELBO\n",
    "\n",
    "Evidence: total probability of observing the data.\n",
    "\n",
    "$\\log p(z)=\\mathcal{L}(q(\\theta))+KL(q(\\theta)||p(\\theta | z))$\n",
    "\n",
    "\n",
    "Notice, KL-divergence is intractable but $KL(q || p) \\geq 0$.\n",
    "\n",
    "\n",
    "$\\log p(z) \\geq \\mathcal{L}(q(\\theta))$\n",
    "\n",
    "#### Now we could formulate an optimization problem with intractable posterior!\n",
    "\n",
    "$\\mathcal{L}(q):= KL(q(\\theta) || p(\\theta | z)) \\rightarrow min_{q(\\theta) \\in \\mathcal{Q}}$\n",
    "\n",
    "$\\mathcal{L}(q(\\theta)) = \\int q(\\theta) \\log \\frac{p(z, \\theta)}{q(\\theta)}d\\theta = \\int q(\\theta)\\log \\frac{p(z|\\theta)p(\\theta)}{q(\\theta)}d\\theta=$\n",
    "\n",
    "$= \\int q(\\theta) \\log p(z |\\theta)d\\theta + \\int q(\\theta)\\log \\frac{p(\\theta)}{q(\\theta)}d\\theta =$\n",
    "\n",
    "$= \\mathbb{E}_{q(\\theta)} \\log p(z | \\theta) - KL(q(\\theta)||p(\\theta))$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\mathbb{E}_{q(\\theta)} \\log p(z | \\theta)$ is called data / measurements term.\n",
    "- $KL(q(\\theta)||p(\\theta))$ is called regularizer\n",
    "\n",
    "Because $KL \\geq 0$, \n",
    "\n",
    "- Either maximize $\\mathbb{E}_{q(\\theta)} \\log p(z | \\theta)$\n",
    "- Or minimize $KL(q(\\theta)||p(\\theta))$\n",
    "\n",
    "Now, the problem will be formulated as:\n",
    "\n",
    "$\\mathcal{L}(q(\\theta)) = \\int q(\\theta) \\log \\frac{p(z, \\theta)}{q(\\theta)}d\\theta \\rightarrow \\max_{q(\\theta) \\in \\mathcal{Q}}$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff663e4f",
   "metadata": {},
   "source": [
    "## How to perform an optimization w.r.t. a distribution?\n",
    "\n",
    "Two Options in general:\n",
    "\n",
    "### 1. Mean Field Approximation (Factorized family)\n",
    "\n",
    "$q(\\theta)=\\prod_{j=1}^mq_j(\\theta_j)$, $\\theta=[\\theta_1, ..., \\theta_m]$\n",
    "\n",
    "Examples: \n",
    "- Mixture Model with Expectation Maximization\n",
    "- Mixture Model with Expectation Propagation\n",
    "\n",
    "\n",
    "### 2. Parametric Approximation (Parametric approximation, also be called as Amoritzed inference)\n",
    "\n",
    "$q(\\theta) = q(\\theta | \\lambda)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c7ecff",
   "metadata": {},
   "source": [
    "## Example: Variational Auto-Encoder\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
