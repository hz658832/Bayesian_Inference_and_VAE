{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6aa7912",
   "metadata": {},
   "source": [
    "# Variational (Bayesian) Inference \n",
    "\n",
    "Probabilistic model: $p(z, \\theta)=p(z|\\theta)p(\\theta)$\n",
    "\n",
    "Training: $p(\\theta | X_{tr}, Y_{tr}) = \\frac{p(Y_{tr}|X_{tr}, \\theta)p(\\theta)}{\\int{p(Y_{tr} | X_{tr}, \\theta)p(\\theta)d\\theta}}$  ===> math. intractable \n",
    "\n",
    "Testing: $p(y|x, X_{tr}, Y_{tr})=\\int{p(y|x, \\theta)p(\\theta | X_{tr}, Y_{tr})}$ ===> math. intractable \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6a0d08",
   "metadata": {},
   "source": [
    "## MCMC\n",
    "Samples from unnormalized $p(\\theta|z)$\n",
    "- Unbiased\n",
    "- Need a looooooot of samples regarding to state space dimension\n",
    "\n",
    "## Variational Inferernce\n",
    "Instead to approximate of $Posterior~p(\\theta|z)$ directly, approximate $p(\\theta|z) \\approx q(\\theta)$, which can be understanden as kind of Representation of a model. So called latent space.\n",
    "- Biased\n",
    "- Faster and more scalable\n",
    "\n",
    "\n",
    "Latent space: refers to an abstract multi-dimensional space containing feature values that we cannot interpret directly, but which encodes a meaningful internal representation of externally observed events.\n",
    "\n",
    "Main Idea: find posterior approximation $p(\\theta|z) \\approx q(\\theta) \\in \\mathcal{Q}$ using the relative entropy (so called Kullback-Leibler divergence) as criterion function:\n",
    "\n",
    "$L(q):= KL(q(\\theta) || p(\\theta | z))$, where $KL(q || p) \\geq 0$\n",
    "\n",
    "Hint:\n",
    "- Entropy of a distribution: $H = - \\sum_{i=1}^Np(x_i)logp(x_i)$\n",
    "- Kullback-Leibler divergence: $KL(q || p) = \\sum_{i=1}^Np(x_i)\\frac{log(p(x_i))}{log(q(x_i))}$\n",
    "\n",
    "\n",
    "Solution: $L(q):= KL(q(\\theta) || p(\\theta | z)) \\rightarrow min_{q(\\theta) \\in \\mathcal{Q}}$\n",
    "\n",
    "Two problems:\n",
    "1. The posterior in the KL can still not be computed\n",
    "2. How to perform an optimization w.r.t. a distribution?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5613953d",
   "metadata": {},
   "source": [
    "## Problem 1: The posterior in the KL can still not be computed\n",
    "\n",
    "### Magic:\n",
    "\n",
    "$\\log p(z) = \\int q(\\theta)\\log p(z)d\\theta = \\int q(\\theta)\\log \\frac{p(z, \\theta)}{p(\\theta|z)}d\\theta$\n",
    "\n",
    "$= \\int q(\\theta) \\log \\frac{p(z, \\theta)q(\\theta)}{p(\\theta | z)q(\\theta)}d\\theta=$\n",
    "\n",
    "$= \\int q(\\theta) \\log \\frac{p(x,\\theta)}{q(\\theta)}d\\theta + \\int q(\\theta)\\log \\frac{q(\\theta)}{p(\\theta | z)}d\\theta =$\n",
    "\n",
    "$=\\mathcal{L}(q(\\theta))+KL(q(\\theta)||p(\\theta | z))$\n",
    "\n",
    "Here:\n",
    "- $=\\mathcal{L}(q(\\theta))$ is called Evidence lower bound (ELBO) or Variational lower Bound\n",
    "- $KL(q(\\theta)||p(\\theta | z))$: KL-divergence needed for VI, which is still, intractable...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed4592b",
   "metadata": {},
   "source": [
    "### ELBO\n",
    "\n",
    "Evidence: total probability of observing the data.\n",
    "\n",
    "$\\log p(z)=\\mathcal{L}(q(\\theta))+KL(q(\\theta)||p(\\theta | z))$\n",
    "\n",
    "\n",
    "Notice, KL-divergence is intractable but $KL(q || p) \\geq 0$.\n",
    "\n",
    "\n",
    "$\\log p(z) \\geq \\mathcal{L}(q(\\theta))$\n",
    "\n",
    "#### Now we could formulate an optimization problem with intractable posterior!\n",
    "\n",
    "$\\mathcal{L}(q):= KL(q(\\theta) || p(\\theta | z)) \\rightarrow min_{q(\\theta) \\in \\mathcal{Q}}$\n",
    "\n",
    "$\\mathcal{L}(q(\\theta)) = \\int q(\\theta) \\log \\frac{p(z, \\theta)}{q(\\theta)}d\\theta = \\int q(\\theta)\\log \\frac{p(z|\\theta)p(\\theta)}{q(\\theta)}d\\theta=$\n",
    "\n",
    "$= \\int q(\\theta) \\log p(z |\\theta)d\\theta + \\int q(\\theta)\\log \\frac{p(\\theta)}{q(\\theta)}d\\theta =$\n",
    "\n",
    "$= \\mathbb{E}_{q(\\theta)} \\log p(z | \\theta) - KL(q(\\theta)||p(\\theta))$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\mathbb{E}_{q(\\theta)} \\log p(z | \\theta)$ is called data / measurements term.\n",
    "- $KL(q(\\theta)||p(\\theta))$ is called regularizer\n",
    "\n",
    "Because $KL \\geq 0$, \n",
    "\n",
    "- Either maximize $\\mathbb{E}_{q(\\theta)} \\log p(z | \\theta)$\n",
    "- Or minimize $KL(q(\\theta)||p(\\theta))$\n",
    "\n",
    "Now, the problem will be formulated as:\n",
    "\n",
    "$\\mathcal{L}(q(\\theta)) = \\int q(\\theta) \\log \\frac{p(z, \\theta)}{q(\\theta)}d\\theta \\rightarrow \\max_{q(\\theta) \\in \\mathcal{Q}}$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff663e4f",
   "metadata": {},
   "source": [
    "## How to perform an optimization w.r.t. a distribution?\n",
    "\n",
    "Two Options in general:\n",
    "\n",
    "### 1. Mean Field Approximation (Factorized family)\n",
    "\n",
    "$q(\\theta)=\\prod_{j=1}^mq_j(\\theta_j)$, $\\theta=[\\theta_1, ..., \\theta_m]$\n",
    "\n",
    "Examples: \n",
    "- Mixture Model with Expectation Maximization\n",
    "- Mixture Model with Expectation Propagation\n",
    "\n",
    "\n",
    "### 2. Parametric Approximation (Parametric approximation, also be called as Amoritzed inference)\n",
    "\n",
    "$q(\\theta) = q(\\theta | \\lambda)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c7ecff",
   "metadata": {},
   "source": [
    "## Example: Variational Auto-Encoder\n",
    "***\n",
    "### Auto Encoder\n",
    "\n",
    "([1] https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73)\n",
    "\n",
    "An idea came for dimensionality reduction for but not limited as data visualisation, data storage, heavy computationâ€¦, same as Single Value Decomposition, Principle Component Analysis ...\n",
    "\n",
    "Idea: Compress the original data with most important / informative features, so that the original data can be reconstructed (original data as label). These features cannot or can very hard be handcrafted defined. \n",
    "\n",
    "Latent space (bottleneck): representation (model) of the most informative features\n",
    "\n",
    "![alt text](figures/AE.png \"Idea of Auto Encoder\")\n",
    "\n",
    "\n",
    "Within AE, for a given set of possible encoders and decoders, we are looking for the pair that keeps the maximum of information when encoding and, so, has the minimum of reconstruction error when decoding.\n",
    "\n",
    "$(enc^*, dnc^*) = argmin_{(enc,dnc) \\in ExD}~\\epsilon (z, dnc(enc(z)))$\n",
    "\n",
    "![alt text](figures/AE_loss.png \"Idea of Auto with loss terms\")\n",
    "\n",
    "#### Limitation: The latent space is not, at least enought, regularized. \n",
    "\n",
    "After training, no new content can be reconstructed with the latent space => strongly overfitted, which against the idea of generative modelling.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdaca15",
   "metadata": {},
   "source": [
    "### Variational Encoder [1]\n",
    "\n",
    "Idea: instead of training a determinstic latent space, train the distribution (normally normal distribution) of the latent space...\n",
    "\n",
    "![alt text](figures/VAE_idea.png \"Idea of Variational Auto Encoder\")\n",
    "\n",
    "![alt text](figures/VAE_loss.png \"Variational Auto Encoder with loss\")\n",
    "\n",
    "### Recap the loss of VBI regarding to ELBO:\n",
    "\n",
    "$\\mathcal{L}(q(\\theta)) = \\int q(\\theta) \\log \\frac{p(z, \\theta)}{q(\\theta)}d\\theta = \\int q(\\theta)\\log \\frac{p(z|\\theta)p(\\theta)}{q(\\theta)}d\\theta=$\n",
    "\n",
    "$= \\mathbb{E}_{q(\\theta)} \\log p(z | \\theta) - KL(q(\\theta)||p(\\theta))$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\mathbb{E}_{q(\\theta)} \\log p(z | \\theta)$ is called data / measurements term.\n",
    "- $KL(q(\\theta)||p(\\theta))$ is called regularizer\n",
    "\n",
    "\n",
    "We see the loss function of VAE, which contains two parts:\n",
    "- reconstruction loss: $|| x - d(z)||^2$, this represents the data term in the loss of VBI\n",
    "- KL-regularizer: $KL(\\mathcal{N}(\\mu_x, \\sigma_x), \\mathcal{N}\\mathbf{(0, I)})$\n",
    "\n",
    "\n",
    "\n",
    "### Intuition about the regularisation \n",
    "\n",
    "![alt text](figures/VAE_Intuition.png \"Variational Auto Encoder: Intuition\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677c0e38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f48e57c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
